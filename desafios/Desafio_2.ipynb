{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
    "#### - Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n",
    "#### - Graficarlos.\n",
    "#### - Obtener conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import os\n",
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np   \n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px                               \n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = 'browser'\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el desafío, usaremos una colección de los 25 libros más descargados de la web de proyecto Gutenberg. Teniendo una lista de ids de libros obtenidas por curl, iteremos sobre la misma para descargar los libros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando libros. . .:  76%|███████▌  | 19/25 [00:37<00:10,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al descargar el libro 38769: 404 Client Error: Not Found for url: https://www.gutenberg.org/cache/epub/38769/pg38769.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando libros. . .: 100%|██████████| 25/25 [00:48<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "book_ids = pd.read_csv(\"data/d2/books_ids.csv\")\n",
    "total_books = len(book_ids)\n",
    "\n",
    "for book_id in tqdm(book_ids['book_id'], desc=\"Descargando libros. . .\", total=total_books):\n",
    "    url = f\"https://www.gutenberg.org/ebooks/{book_id}.txt.utf-8\"\n",
    "    output_dir= \"data/d2/books\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Lanza un error si la descarga falla\n",
    "        with open(os.path.join(output_dir, f\"{book_id}.txt\"), 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el libro {book_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos cada uno de los .txt y segmentamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447562\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Project Gutenberg eBook of The Complete Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This ebook is for the use of anyone anywhere i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>most other parts of the world at no cost and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whatsoever. You may copy it, give it away or r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of the Project Gutenberg License included with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence\n",
       "0  The Project Gutenberg eBook of The Complete Wo...\n",
       "1  This ebook is for the use of anyone anywhere i...\n",
       "2  most other parts of the world at no cost and w...\n",
       "3  whatsoever. You may copy it, give it away or r...\n",
       "4  of the Project Gutenberg License included with..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for file in os.listdir(output_dir):\n",
    "    if file.endswith('.txt'):\n",
    "        dfx = pd.read_csv(os.path.join(output_dir, file), sep='/n', header=None, names=['Sentence'], engine='python')\n",
    "        dfs.append(dfx)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamos el dataset como vimos en clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = []\n",
    "# Recorrer todas las filas y transformar las oraciones\n",
    "# en una secuencia de palabras (esto podría realizarse con NLTK o spaCy también)\n",
    "for _, row in df.iloc[:].iterrows():\n",
    "    sentence_tokens.append(text_to_word_sequence(row.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos nuestra vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=5,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
    "                     window=2,       # cant de palabras antes y desp de la predicha\n",
    "                     vector_size=300,       # dimensionalidad de los vectores \n",
    "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
    "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
    "                     sg=1)           # modelo 0:CBOW  1:skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de docs en el corpus: 447562\n",
      "Cantidad de words distintas en el corpus: 24699\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentence_tokens)\n",
    "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)\n",
    "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:   5%|▌         | 1/20 [00:37<12:01, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 20991722.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  10%|█         | 2/20 [01:16<11:25, 38.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 20474404.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  15%|█▌        | 3/20 [01:55<10:58, 38.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 20275872.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  20%|██        | 4/20 [02:34<10:19, 38.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 20131648.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  25%|██▌       | 5/20 [03:13<09:41, 38.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19996540.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  30%|███       | 6/20 [03:52<09:04, 38.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19868238.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  35%|███▌      | 7/20 [04:33<08:37, 39.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19761020.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  40%|████      | 8/20 [05:12<07:53, 39.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19674336.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  45%|████▌     | 9/20 [05:51<07:13, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19580736.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  50%|█████     | 10/20 [06:31<06:33, 39.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19491266.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  55%|█████▌    | 11/20 [07:10<05:53, 39.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19410360.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  60%|██████    | 12/20 [07:49<05:13, 39.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19336386.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  65%|██████▌   | 13/20 [08:28<04:34, 39.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19274156.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  70%|███████   | 14/20 [09:07<03:54, 39.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19193748.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  75%|███████▌  | 15/20 [09:45<03:14, 38.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19118222.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  80%|████████  | 16/20 [10:24<02:34, 38.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19068208.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  85%|████████▌ | 17/20 [11:35<02:25, 48.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 19020818.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  90%|█████████ | 18/20 [12:14<01:31, 45.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 18947492.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .:  95%|█████████▌| 19/20 [12:53<00:43, 43.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 18912818.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando . . .: 100%|██████████| 20/20 [13:34<00:00, 40.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 18853790.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(20), desc=\"Entrenando . . .\", total=20):\n",
    "    w2v_model.train(sentence_tokens,\n",
    "                     total_examples=w2v_model.corpus_count,\n",
    "                     epochs=1,\n",
    "                     compute_loss=True,\n",
    "                     callbacks=[callback()]\n",
    "                     )\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizamos similitudes en espacio de embedding como vimos en clase, comparando las caracteristicas de cada término en coseno similitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.5008445382118225),\n",
       " ('clerval', 0.4751327335834503),\n",
       " ('stepmother', 0.4726009666919708),\n",
       " ('lucie', 0.4715188443660736),\n",
       " ('ratchcali', 0.4673488438129425),\n",
       " ('ottilie', 0.46160653233528137),\n",
       " ('justine', 0.46006518602371216),\n",
       " ('espousing', 0.45555177330970764),\n",
       " ('rosalie', 0.45282384753227234),\n",
       " ('gordons', 0.4520285129547119)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"father\"], topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de \"father\" podemos ver palabras que son similares ya que representan un vínculo familair ascendente, y otros términos relativos a los libros con que entrenamos (Nombres de personajes que son padres y madres, que en el corpus se usan como términos similares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('forborne', 0.46100419759750366),\n",
       " ('luggage', 0.45130211114883423),\n",
       " ('cash', 0.4449760913848877),\n",
       " ('funds', 0.4422464668750763),\n",
       " ('premium', 0.43783068656921387),\n",
       " ('herewith', 0.4362329840660095),\n",
       " ('dollar', 0.4298473000526428),\n",
       " ('invoice', 0.4291066825389862),\n",
       " ('eighty', 0.4268193542957306),\n",
       " ('quarts', 0.4251228868961334)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"money\"], topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos la cercanía de los terminos reduciendo la dimensionalidad de embedding con PCA, con el fin de poder acomodarlos en gráficas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_model, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos un gráfico tridimensional que posiciona los términos basándose en las tres características de PCA:\n",
    "\n",
    "_(Nota: el gráfico se abrirá en navegador web)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_WORDS = 200  \n",
    "fig = px.scatter_3d(x=vecs[:MAX_WORDS, 0], y=vecs[:MAX_WORDS, 1], z=vecs[:MAX_WORDS, 2], text=labels[:MAX_WORDS])\n",
    "fig.update_traces(marker_size = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
