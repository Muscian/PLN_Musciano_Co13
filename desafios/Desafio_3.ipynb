{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 3\n",
    "\n",
    "### Consigna\n",
    "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
    "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
    "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
    "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
    "\n",
    "_Se realuizarán las consignas tanto para un modelo de lenguaje de caracteres como uno de palabras, como hemos visto en clase._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Código Preliminar\n",
    "_<span style=\"font-size:smaller;\">Imports y configuración.</span>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  Código Preliminar  #####\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import keras\n",
    "from keras.layers import SimpleRNN, Dense, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "books_directory = \"data/d3/books\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección del Corpus\n",
    "_<span style=\"font-size:smaller;\">Volveremos a usar libros del Gudenberg Project, esta vez una mayor cantidad.</span>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ids = pd.read_csv(\"data/d3/books_ids.csv\")\n",
    "total_books = len(book_ids)\n",
    "\n",
    "for book_id in tqdm(book_ids['book_id'], desc=\"Descargando libros. . .\", total=total_books):\n",
    "    url = f\"https://www.gutenberg.org/ebooks/{book_id}.txt.utf-8\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Lanza un error si la descarga falla\n",
    "        with open(os.path.join(books_directory, f\"{book_id}.txt\"), 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el libro {book_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-procesado del Corpus\n",
    "_<span style=\"font-size:smaller;\">Unificamos los documentos en un solo corpus y lo tokenizamos</span>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos un corpus de 4024032 caracteres\n",
      "Y un vocabulario de 118.\n"
     ]
    }
   ],
   "source": [
    "# Leemos todos los textos y los almacenamos en una lista\n",
    "corpus = []\n",
    "for filename in os.listdir(books_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(books_directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
    "            # Leer el contenido del archivo y agregarlo a la lista\n",
    "            content = file.read()\n",
    "            corpus.append(content)\n",
    "\n",
    "# Concatenamos todos los textos en una sola cadena\n",
    "corpus_text = ' '.join(corpus)\n",
    "\n",
    "max_context_size = 100\n",
    "chars_vocab = set(corpus_text)\n",
    "print(f\"Tenemos un corpus de {len(corpus_text)} caracteres\")\n",
    "print(f\"Y un vocabulario de {len(chars_vocab)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(sorted(chars_vocab))}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_context_size = 100  # El tamaño de la secuencia de entrada\n",
    "corpus_length = len(corpus_text)\n",
    "vocab_size = len(chars_vocab)\n",
    "\n",
    "# Inicializar matrices de entrada y salida\n",
    "input_sequences = np.zeros((corpus_length - max_context_size, max_context_size), dtype=np.int32)\n",
    "target_sequences = np.zeros(corpus_length - max_context_size, dtype=np.int32)\n",
    "\n",
    "# Rellenar matrices con índices correspondientes\n",
    "for i in tqdm(range(corpus_length - max_context_size)):\n",
    "    input_sequences[i] = [char_to_idx[char] for char in corpus_text[i: i + max_context_size]]\n",
    "    target_sequences[i] = char_to_idx[corpus_text[i + max_context_size]]\n",
    "\n",
    "print(f\"Generamos {input_sequences.shape[0]} secuencias de entrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructuración del Dataset\n",
    "_<span style=\"font-size:smaller;\">Dividimos en train y test como vimos en clase.</span>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(input_sequences)\n",
    "y = np.array(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "vocab_size = len(chars_vocab)\n",
    "embedding_dim = 50  # Puedes ajustar este valor\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_context_size))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "_<span style=\"font-size:smaller;\">El resultado se guardará en la carpeta /models.</span>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=4, batch_size=64)\n",
    "model.save('models/lang_char.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del modelo\n",
    "\n",
    "_<span style=\"font-size:smaller;\">Para no entrenar en cada ejecución.</span>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/lang_char.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de Secuencias - Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcionalidades para hacer encoding y decoding\n",
    "\n",
    "def encode(input_text):\n",
    "    encoded = [char_to_idx[char] for char in input_text]\n",
    "    return np.array([encoded])  # Devolver una matriz 2D con forma (1, sequence_length)\n",
    "\n",
    "def decode(seq):\n",
    "    return ''.join([idx_to_char[ch] for ch in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, num_beams, num_words, input, temp=1.0, mode='det'):\n",
    "    # Primera iteración\n",
    "\n",
    "    # Codificar\n",
    "    encoded = encode(input)\n",
    "\n",
    "    # Primera predicción\n",
    "    y_hat = model.predict(encoded, verbose=0)[0, :]\n",
    "\n",
    "    # Obtener el tamaño del vocabulario\n",
    "    vocab_size = y_hat.shape[0]\n",
    "\n",
    "    # Inicializar historial\n",
    "    history_probs = [0] * num_beams\n",
    "    history_tokens = [encoded[0]] * num_beams\n",
    "\n",
    "    # Seleccionar candidatos\n",
    "    history_probs, history_tokens = select_candidates([y_hat],\n",
    "                                                      num_beams,\n",
    "                                                      vocab_size,\n",
    "                                                      history_probs,\n",
    "                                                      history_tokens,\n",
    "                                                      temp,\n",
    "                                                      mode)\n",
    "\n",
    "    # Bucle de búsqueda en haz\n",
    "    for i in range(num_words - 1):\n",
    "        preds = []\n",
    "\n",
    "        for hist in history_tokens:\n",
    "            # Actualizar secuencia de tokens\n",
    "            input_update = np.array([hist[-max_context_size:]])\n",
    "\n",
    "            # Predicción\n",
    "            y_hat = model.predict(input_update, verbose=0)[0, :]\n",
    "\n",
    "            # Aplicar la temperatura a las probabilidades antes de añadir a preds\n",
    "            y_hat = np.log(y_hat + 1e-10) / temp\n",
    "            y_hat = np.exp(y_hat) / np.sum(np.exp(y_hat))  # Softmax con temperatura\n",
    "\n",
    "            preds.append(y_hat)\n",
    "\n",
    "        history_probs, history_tokens = select_candidates(preds,\n",
    "                                                          num_beams,\n",
    "                                                          vocab_size,\n",
    "                                                          history_probs,\n",
    "                                                          history_tokens,\n",
    "                                                          temp,\n",
    "                                                          mode)\n",
    "\n",
    "    return history_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is another, while \n",
      "He is they were they \n",
      "He is thought though \n",
      "He is another,” said \n",
      "He is anotherwisfulle\n"
     ]
    }
   ],
   "source": [
    "# Observemos salidas con distintas temperaturas\n",
    "print(decode(beam_search(model,num_beams=10,num_words=15,input=\"He is \", temp=1)[0]))\n",
    "print(decode(beam_search(model,num_beams=10,num_words=15,input=\"He is \", temp=0.5)[0]))\n",
    "print(decode(beam_search(model,num_beams=10,num_words=15,input=\"He is \", temp=2)[0]))\n",
    "print(decode(beam_search(model,num_beams=10,num_words=15,input=\"He is \", temp=10)[0]))\n",
    "print(decode(beam_search(model,num_beams=10,num_words=15,input=\"He is \", temp=20)[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search(model, input_text, num_words, temp=1.0):\n",
    "    # Codificar el texto de entrada\n",
    "    encoded = encode(input_text)\n",
    "    \n",
    "    # Inicializar la secuencia generada\n",
    "    generated_sequence = list(encoded[0])\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        # Preparar la entrada para el modelo\n",
    "        input_sequence = np.array([generated_sequence[-max_context_size:]])\n",
    "        \n",
    "        # Obtener la predicción del modelo\n",
    "        y_hat = model.predict(input_sequence, verbose=0)[0, :]\n",
    "        \n",
    "        # Aplicar la temperatura a las probabilidades\n",
    "        y_hat = np.log(y_hat + 1e-10) / temp\n",
    "        y_hat = np.exp(y_hat) / np.sum(np.exp(y_hat))  # Softmax con temperatura\n",
    "\n",
    "        # Seleccionar el índice con la mayor probabilidad\n",
    "        next_char_idx = np.argmax(y_hat)\n",
    "        \n",
    "        # Agregar el siguiente carácter a la secuencia generada\n",
    "        generated_sequence.append(next_char_idx)\n",
    "\n",
    "    return generated_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a stranger of the\n",
      "\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "# Definir el texto de entrada y el número de caracteres a generar\n",
    "input_text = \"He is \"\n",
    "num_words = 20\n",
    "\n",
    "# Ejecutamos la greedy\n",
    "result_sequence = greedy_search(model, input_text, num_words, 0.1)\n",
    "\n",
    "# Decodificamos la secuencia generada\n",
    "generated_text = decode(result_sequence)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentación / Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamaño del vocabulario es: 31515\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Creamos un tokenizer para convertir las palabras a índices\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([corpus_text])\n",
    "\n",
    "# Convertir el texto completo en una secuencia de índices de palabras\n",
    "sequences = tokenizer.texts_to_sequences([corpus_text])[0]\n",
    "\n",
    "# Obtener el tamaño del vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 por el token de padding\n",
    "\n",
    "print(f\"El tamaño del vocabulario es: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de Secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generamos 709430 secuencias de entrada.\n"
     ]
    }
   ],
   "source": [
    "max_context_size = 10  # Tamaño de la secuencia de entrada\n",
    "\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "\n",
    "for i in range(0, len(sequences) - max_context_size):\n",
    "    input_seq = sequences[i: i + max_context_size]\n",
    "    target_word = sequences[i + max_context_size]\n",
    "    input_sequences.append(input_seq)\n",
    "    target_sequences.append(target_word)\n",
    "\n",
    "# Convertir a arrays de NumPy\n",
    "X = np.array(input_sequences)\n",
    "y = np.array(target_sequences)\n",
    "\n",
    "print(f\"Generamos {len(input_sequences)} secuencias de entrada.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcción del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "embedding_dim = 50  # Puedes ajustar este valor\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 47ms/step - accuracy: 0.0634 - loss: 6.9927\n",
      "Epoch 2/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 47ms/step - accuracy: 0.1140 - loss: 6.0684\n",
      "Epoch 3/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 47ms/step - accuracy: 0.1284 - loss: 5.7463\n",
      "Epoch 4/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 47ms/step - accuracy: 0.1393 - loss: 5.5229\n",
      "Epoch 5/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 47ms/step - accuracy: 0.1475 - loss: 5.3444\n",
      "Epoch 6/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 47ms/step - accuracy: 0.1551 - loss: 5.2018\n",
      "Epoch 7/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 47ms/step - accuracy: 0.1608 - loss: 5.0886\n",
      "Epoch 8/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 47ms/step - accuracy: 0.1674 - loss: 4.9732\n",
      "Epoch 9/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 47ms/step - accuracy: 0.1734 - loss: 4.8620\n",
      "Epoch 10/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 47ms/step - accuracy: 0.1795 - loss: 4.7477\n",
      "Epoch 11/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 47ms/step - accuracy: 0.1853 - loss: 4.6461\n",
      "Epoch 12/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 48ms/step - accuracy: 0.1917 - loss: 4.5474\n",
      "Epoch 13/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 48ms/step - accuracy: 0.1982 - loss: 4.4541\n",
      "Epoch 14/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 48ms/step - accuracy: 0.2053 - loss: 4.3679\n",
      "Epoch 15/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 53ms/step - accuracy: 0.2106 - loss: 4.2869\n",
      "Epoch 16/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 53ms/step - accuracy: 0.2176 - loss: 4.2055\n",
      "Epoch 17/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 53ms/step - accuracy: 0.2250 - loss: 4.1368\n",
      "Epoch 18/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 54ms/step - accuracy: 0.2311 - loss: 4.0688\n",
      "Epoch 19/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 55ms/step - accuracy: 0.2374 - loss: 4.0001\n",
      "Epoch 20/20\n",
      "\u001b[1m11085/11085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 56ms/step - accuracy: 0.2443 - loss: 3.9419\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=64)\n",
    "model.save('models/lang_word.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/lang_word.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generación de Secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, num_beams, num_words, input_text, temp=1.0, mode='det'):\n",
    "    # Codificar el texto de entrada\n",
    "    encoded = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    generated_sequence = list(encoded)\n",
    "\n",
    "    # Inicializar historial\n",
    "    history_probs = [0] * num_beams\n",
    "    history_tokens = [generated_sequence] * num_beams\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        preds = []\n",
    "\n",
    "        for hist in history_tokens:\n",
    "            input_sequence = pad_sequences([hist[-max_context_size:]], maxlen=max_context_size)\n",
    "            \n",
    "            # Obtener la predicción del modelo\n",
    "            y_hat = model.predict(input_sequence, verbose=0)[0, :]\n",
    "            \n",
    "            # Aplicar la temperatura a las probabilidades\n",
    "            y_hat = np.log(y_hat + 1e-10) / temp\n",
    "            y_hat = np.exp(y_hat) / np.sum(np.exp(y_hat))  # Softmax con temperatura\n",
    "\n",
    "            preds.append(y_hat)\n",
    "\n",
    "        history_probs, history_tokens = select_candidates(preds,\n",
    "                                                          num_beams,\n",
    "                                                          vocab_size,\n",
    "                                                          history_probs,\n",
    "                                                          history_tokens,\n",
    "                                                          temp,\n",
    "                                                          mode)\n",
    "\n",
    "    return history_tokens\n",
    "\n",
    "# Función de decodificación para convertir de índices a texto\n",
    "def decode(sequence):\n",
    "    return ' '.join([tokenizer.index_word[idx] for idx in sequence if idx in tokenizer.index_word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Search (Temp = 1.0):\n",
      "he is a weary ” —sir henry eyeing him as a bridal and the upper end of the country and the sailors\n",
      "\n",
      "Beam Search (Temp = 0.5):\n",
      "he is a weary ” —sir henry eyeing him as a bridal and the upper end of the country and the sailors\n",
      "\n",
      "Beam Search (Temp = 1.5):\n",
      "he is a weary ” —sir henry eyeing him as a bridal and the upper end of the country and the sailors\n"
     ]
    }
   ],
   "source": [
    "num_beams = 3\n",
    "\n",
    "# Generación con temperatura 1.0 (predeterminada)\n",
    "result_tokens = beam_search(model, num_beams, num_words, input_text, temp=1.0)\n",
    "generated_text_default = decode(result_tokens[0])\n",
    "print(\"Beam Search (Temp = 1.0):\")\n",
    "print(generated_text_default)\n",
    "\n",
    "# Generación con temperatura 0.5\n",
    "result_tokens = beam_search(model, num_beams, num_words, input_text, temp=0.5)\n",
    "generated_text_low_temp = decode(result_tokens[0])\n",
    "print(\"\\nBeam Search (Temp = 0.5):\")\n",
    "print(generated_text_low_temp)\n",
    "\n",
    "# Generación con temperatura 1.5\n",
    "result_tokens = beam_search(model, num_beams, num_words, input_text, temp=1.5)\n",
    "generated_text_high_temp = decode(result_tokens[0])\n",
    "print(\"\\nBeam Search (Temp = 1.5):\")\n",
    "print(generated_text_high_temp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
